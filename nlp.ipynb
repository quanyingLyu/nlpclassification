{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from BBPETokenizer import BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n",
      "1208827\n"
     ]
    }
   ],
   "source": [
    "with open('COMP90042_2024-main/data/train-claims.json') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('COMP90042_2024-main\\data\\dev-claims.json') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "with open('COMP90042_2024-main\\data\\dev-claims-baseline.json') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open('COMP90042_2024-main\\data\\evidence.json') as f:\n",
    "    evidence_data = json.load(f)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(evidence_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "def processText(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "\n",
    "    return ' '.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load('bbpe.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(claim, evidence, max_length):\n",
    "    special_dict = tokenizer.special_voca()\n",
    "    result = tokenizer.encode('[CLS]' + claim + '[SEP]' + evidence + '[SEP]')\n",
    "    if len(result) > max_length*2:\n",
    "        claim_encode = tokenizer.encode(claim)\n",
    "        evidence_encode = tokenizer.encode(evidence)\n",
    "        if len(claim_encode) > max_length:\n",
    "            claim_encode = claim_encode[:max_length-1]\n",
    "            if len(evidence_encode) > max_length:\n",
    "                evidence_encode = evidence_encode[:max_length-1]\n",
    "        else:\n",
    "            if len(evidence_encode) > max_length:\n",
    "                evidence_encode = evidence_encode[:max_length*2-len(claim_encode)-2]\n",
    "        result = claim_encode+ [special_dict['[SEP]']] + evidence_encode + [special_dict['[SEP]']]\n",
    "        if len(result) < max_length*2:\n",
    "            result += [special_dict['[PAD]']] * (max_length*2 - len(result))\n",
    "    else:\n",
    "        result += [special_dict['[PAD]']] * (max_length*2 - len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"SUPPORTS\", \"NOT_ENOUGH_INFO\", \"REFUTES\", \"DISPUTED\"]\n",
    "labelsid = {\"SUPPORTS\": 0, \"NOT_ENOUGH_INFO\": 1, \"REFUTES\": 2, \"DISPUTED\": 3}\n",
    "specials = ['[CLS]', '[SEP]', '[PAD]', '[UNK]', '[MASKED]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claim-1937</td>\n",
       "      <td>scientific evidence co2 pollutant higher co2 c...</td>\n",
       "      <td>high concentrations 100 times atmospheric conc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claim-1937</td>\n",
       "      <td>scientific evidence co2 pollutant higher co2 c...</td>\n",
       "      <td>higher carbon dioxide concentrations favourabl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim-1937</td>\n",
       "      <td>scientific evidence co2 pollutant higher co2 c...</td>\n",
       "      <td>high concentrations 100 times atmospheric conc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>claim-126</td>\n",
       "      <td>el niño drove record highs global temperatures...</td>\n",
       "      <td>climate change due natural forces human activi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claim-126</td>\n",
       "      <td>el niño drove record highs global temperatures...</td>\n",
       "      <td>acceleration due mostly humancaused global war...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        claim                                               text  \\\n",
       "0  claim-1937  scientific evidence co2 pollutant higher co2 c...   \n",
       "1  claim-1937  scientific evidence co2 pollutant higher co2 c...   \n",
       "2  claim-1937  scientific evidence co2 pollutant higher co2 c...   \n",
       "3   claim-126  el niño drove record highs global temperatures...   \n",
       "4   claim-126  el niño drove record highs global temperatures...   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  high concentrations 100 times atmospheric conc...      3  \n",
       "1  higher carbon dioxide concentrations favourabl...      3  \n",
       "2  high concentrations 100 times atmospheric conc...      3  \n",
       "3  climate change due natural forces human activi...      2  \n",
       "4  acceleration due mostly humancaused global war...      2  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data = []\n",
    "processed_test_data = []\n",
    "\n",
    "for id, values in train_data.items():\n",
    "    evidences = ''\n",
    "    for evidence in values['evidences']:\n",
    "        evidences += processText(evidence_data[evidence]) + '[SEP]'\n",
    "    processed_train_data.append([id, processText(values['claim_text']), evidences, labelsid[values['claim_label']]])\n",
    "\n",
    "    evidences = ''\n",
    "    shuffled_evidences = values['evidences'].copy() \n",
    "    random.shuffle(shuffled_evidences)  \n",
    "    for evidence in shuffled_evidences:\n",
    "            evidences += processText(evidence_data[evidence]) + '[SEP]'\n",
    "    processed_train_data.append([id, processText(values['claim_text']), evidences, labelsid[values['claim_label']]])\n",
    "\n",
    "    if labelsid[values['claim_label']] in [1, 2, 3]:\n",
    "        if random.random() < 0.3 and len(values['evidences']) > 1: \n",
    "            evidences = ''\n",
    "            shuffled_evidences = values['evidences'].copy() \n",
    "            random.shuffle(shuffled_evidences)  \n",
    "            for evidence in shuffled_evidences:\n",
    "                evidences += processText(evidence_data[evidence]) + '[SEP]'\n",
    "            processed_train_data.append([id, processText(values['claim_text']), evidences, labelsid[values['claim_label']]])\n",
    "\n",
    "df_train_data = pd.DataFrame(processed_train_data, columns=['claim', 'text', 'evidence', 'label'])\n",
    "\n",
    "\n",
    "for id, values in test_data.items():\n",
    "    evidences = ''\n",
    "    for evidence in values['evidences']:\n",
    "        evidences += processText(evidence_data[evidence]) + '[SEP]'\n",
    "    processed_test_data.append([id, processText(values['claim_text']), evidences, labelsid[values['claim_label']]])\n",
    "df_test_data = pd.DataFrame(processed_test_data, columns=['claim', 'text', 'evidence', 'label'])\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Counts:\n",
      "0    1038\n",
      "1     879\n",
      "2     440\n",
      "3     273\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Label Counts:\n",
      "0    68\n",
      "1    41\n",
      "2    27\n",
      "3    18\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_categories = [0, 1, 2, 3]\n",
    "\n",
    "# 统计训练集各标签的数量，使用 reindex 确保每个种类都显示\n",
    "train_label_counts = df_train_data['label'].value_counts().reindex(label_categories, fill_value=0)\n",
    "print(\"Training Label Counts:\")\n",
    "print(train_label_counts)\n",
    "\n",
    "# 统计测试集各标签的数量，使用 reindex 确保每个种类都显示\n",
    "test_label_counts = df_test_data['label'].value_counts().reindex(label_categories, fill_value=0)\n",
    "print(\"\\nTest Label Counts:\")\n",
    "print(test_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, data, label_coloum):\n",
    "        super().__init__()\n",
    "        self.dataframe = data\n",
    "        self.label = label_coloum\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        text_tensor = torch.tensor(encoding(row['text'], row['evidence'], 256), dtype=torch.long)  # 转换为 tensor\n",
    "        label_tensor = torch.tensor(row[self.label], dtype=torch.long)  # 也转换为 tensor\n",
    "        return text_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coloum = 'label'\n",
    "\n",
    "train_dataset = ClassificationDataset(df_train_data, label_coloum=label_coloum)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = ClassificationDataset(df_test_data, label_coloum=label_coloum)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):  # Batch first!\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        # Create a matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Initialize the position indices (0 to max_len - 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        # Create a divisor term based on the exponential of the dimension indices\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # Shape: (d_model // 2,)\n",
    "        # Compute sine and cosine encodings for even and odd indices respectively\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Shape: (max_len, d_model // 2)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Shape: (max_len, d_model // 2)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input tensor `x`.\n",
    "        \n",
    "        Parameters:\n",
    "        - `x`: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor with positional encoding added to `x`.\n",
    "        \"\"\"\n",
    "        # x: shape (batch_size, seq_len, d_model)\n",
    "        # self.pe: shape (1, max_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RopePositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):    \n",
    "        if self.alpha is not None:\n",
    "            self.alpha = self.alpha.to(targets.device)\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 计算 pt，即预测正确的概率\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            apha_t = self.alpha[targets]\n",
    "        else:\n",
    "            apha_t = 0.75\n",
    "        # 计算 Focal Loss\n",
    "        focal_loss = apha_t * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        # 根据 reduction 参数进行不同的返回\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, nhead, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = hidden_dim//nhead\n",
    "        \n",
    "        self.Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, X, attention_masked=None):\n",
    "        batch, seq_len, _ = X.size()\n",
    "        \n",
    "        Q = self.Q(X)\n",
    "        K = self.K(X)\n",
    "        V = self.V(X)\n",
    "        \n",
    "        Q = Q.view(batch, seq_len, self.nhead, self.head_dim).transpose(1,2)\n",
    "        K = K.view(batch, seq_len, self.nhead, self.head_dim).transpose(1,2)\n",
    "        V = V.view(batch, seq_len, self.nhead, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        attention_score = Q@K.transpose(-1,-2)/math.sqrt(self.head_dim)\n",
    "        \n",
    "        if attention_masked is not None:\n",
    "            attention_score = attention_score.masked_fill(attention_masked == 0, float('-inf'))\n",
    "        attention_score = torch.softmax(attention_score, -1)\n",
    "        attention_score= self.dropout(attention_score)\n",
    "        \n",
    "        output = attention_score @ V\n",
    "        \n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.view(batch, seq_len, -1)\n",
    "        \n",
    "        output = self.output(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, nhead, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = MultiheadAttention(hidden_dim, nhead, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),  \n",
    "            nn.ReLU(), \n",
    "\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_masked=None):\n",
    "\n",
    "        attn_output = self.multihead_attn(X, attention_masked)\n",
    "        X = self.layer_norm1(X + self.dropout(attn_output))  \n",
    "\n",
    "        ffn_output = self.ffn(X)\n",
    "        X = self.layer_norm2(X + self.dropout(ffn_output)) \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, nhead, num_layers, num_classes=4,dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "        self.encoder_layer = nn.ModuleList([EncoderLayer(hidden_dim, nhead, dropout_rate) for _ in range(num_layers)])\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_id):\n",
    "        X = self.embedding(input_id)\n",
    "        X = self.positional_encoding(X)\n",
    "\n",
    "        for layer in self.encoder_layer:\n",
    "            X = layer(X)\n",
    "            \n",
    "        X_cls = X[:, 0, :] \n",
    "        logits = self.classifier(X_cls) \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df_train_data['label'].value_counts().sort_index()\n",
    "total_samples = label_counts.sum()  # 计算总样本数\n",
    "\n",
    "# 计算每个 label 的比例\n",
    "label_ratios = label_counts / total_samples\n",
    "label_ratios1 = [1,1,1.5,1.5]\n",
    "alpha_tensor = torch.tensor(label_ratios1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.voca_size()\n",
    "embed_dim = 512# Dimension of the embedding layer\n",
    "lr = 1e-5\n",
    "\n",
    "model = ClassificationModel(vocab_size, embed_dim, nhead=4, num_layers=1, num_classes=4, dropout_rate=0.1 )\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=0.0001)\n",
    "focal_loss = FocalLoss(alpha=alpha_tensor, gamma=1.5, reduction='mean')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 100%|██████████| 83/83 [00:30<00:00,  2.72it/s]\n",
      "Epoch 1/20 - Validation: 100%|██████████| 5/5 [00:02<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.9485, Validation Loss: 0.8756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 100%|██████████| 83/83 [00:52<00:00,  1.60it/s]\n",
      "Epoch 2/20 - Validation: 100%|██████████| 5/5 [00:02<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.8367, Validation Loss: 0.7741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training:  10%|▉         | 8/83 [00:04<00:44,  1.69it/s]"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 将模型和损失函数移到 GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "focal_loss = focal_loss.to(device)\n",
    "\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "n_epochs_stop = 3\n",
    "num_epochs = 20\n",
    "best_model_state_dict = None\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training Loop with Early Stopping and tqdm progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # 使用 tqdm 包裹 dataloader，显示训练进度\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = focal_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = focal_loss(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(test_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # 调整学习率\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early Stopping 检查\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_state_dict = model.state_dict()  # 保存最佳模型\n",
    "        epochs_no_improve = 0\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve >= n_epochs_stop:\n",
    "    #         print(\"Early stopping!\")\n",
    "    #         break\n",
    "\n",
    "# 绘制训练和验证损失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.3469\n",
      "加权 F1 Score (Weighted F1): 0.3632\n",
      "微平均 F1 Score (Micro F1): 0.4351\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_model_state_dict)\n",
    "model.eval()  # 设置模型为评估模式\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    for batch in test_dataloader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 模型预测\n",
    "        outputs = model(inputs)  # 输出 logits\n",
    "        probs = torch.softmax(outputs, dim=1)  # 转化为概率\n",
    "\n",
    "        # 获取预测标签\n",
    "        _, preds = torch.max(probs, dim=1)\n",
    "\n",
    "        # 收集真实标签和预测结果\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# 转换为 numpy 数组\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# 计算 F1 分数（对于多分类任务可以设置 average='macro' 或 'weighted'）\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')  # 'macro' 适用于多分类\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f\"加权 F1 Score (Weighted F1): {weighted_f1:.4f}\")\n",
    "\n",
    "# 微平均 F1 Score（micro F1）\n",
    "micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "print(f\"微平均 F1 Score (Micro F1): {micro_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试结果:\n",
    "\n",
    "\n",
    "#### 绝对位置编码：\n",
    "\n",
    "##### Focal Loss:\n",
    "\n",
    "\n",
    "**alpha = 0.75         gamma = 2:         mean**  \n",
    "hidden_dim = 512     nhead = 4       num_layers = 4      dropout = 0.1       lr = 0.001       f1 = 0.3007        weighted_f1 = 0.4197     micro_f1 = 0.5130\n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 2      dropout = 0.1       lr = 0.001       f1 = 0.3386        weighted_f1 = 0.4715     micro_f1 = 0.5714    \n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 1      dropout = 0.1       lr = 0.001       f1 = 0.3519        weighted_f1 = 0.4909      micro_f1 = 0.5909\n",
    "\n",
    "\n",
    "**alpha = 1,1,1.5,1.5         gamma = 2:         mean**\n",
    "hidden_dim = 512     nhead = 4       num_layers = 1      dropout = 0.1       lr = 0.001       f1 = 0.3623        weighted_f1 = 0.4957      micro_f1 = 0.5844\n",
    "\n",
    "\n",
    "\n",
    "数据增强\n",
    "**alpha = 1,1,1.5,1.5         gamma = 2:         mean 少量数据增加30%**\n",
    "hidden_dim = 512     nhead = 4       num_layers = 4      dropout = 0.1       lr = 0.001       f1 = 0.3469        weighted_f1 = 0.3632      micro_f1 = 0.4351\n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 2      dropout = 0.1       lr = 0.001       f1 = 0.4567        weighted_f1 = 0.5227      micro_f1 = 0.5649\n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 1      dropout = 0.1       lr = 0.001       f1 = 0.4193        weighted_f1 = 0.4971      micro_f1 = 0.5455\n",
    "\n",
    "**alpha = 1,1,1.5,1.5         gamma = 2:         mean 双倍所有数据，并且少量数据增加30%**.\n",
    "hidden_dim = 512     nhead = 4       num_layers = 4      dropout = 0.1       lr = 0.001       f1 =         weighted_f1 =       micro_f1 = \n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 2      dropout = 0.1       lr = 0.001       f1 =         weighted_f1 =       micro_f1 = \n",
    "\n",
    "hidden_dim = 512     nhead = 4       num_layers = 1      dropout = 0.1       lr = 0.001       f1 =         weighted_f1 =       micro_f1 = \n",
    "\n",
    "\n",
    "\n",
    "#### Rope位置编码：\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4973841,
     "sourceId": 8367260,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
